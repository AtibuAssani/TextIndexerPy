import os
import re


def lire_documents(dossier):
    """
    Lit tous les fichiers .txt dans le dossier donnÃ©.
    Retourne un dictionnaire : {nom_fichier: contenu}
    """
    documents = {}
    for fichier in os.listdir(dossier):
        if fichier.endswith(".txt"):
            chemin_complet = os.path.join(dossier, fichier)
            print(f"ğŸ“„ Chargement du fichier : {fichier}")  # âœ… LIGNE AJOUTÃ‰E
            try:
                with open(chemin_complet, 'r', encoding='utf-8') as f:
                    documents[fichier] = f.read()
            except Exception as e:
                print(f"âŒ Erreur de lecture de {fichier} : {e}")
    print(f"ğŸ“ Total : {len(documents)} fichiers chargÃ©s.\n")  # âœ… FIN DE FONCTION
    return documents



def pretraitement(texte):
    """
    Met le texte en minuscules et enlÃ¨ve la ponctuation.
    """
    texte = texte.lower()
    texte = re.sub(r'[^\w\s]', '', texte)
    return texte


def tokenisation(texte):
    """
    DÃ©coupe le texte en mots (tokens) en utilisant les espaces.
    """
    tokens = texte.split()
    return tokens


def charger_et_traiter(dossier):
    """
    Charge et traite tous les fichiers texte dans le dossier donnÃ©.
    Retourne un dictionnaire : {nom_fichier: liste de tokens}
    """
    documents_bruts = lire_documents(dossier)
    documents_tokenises = {}
    for nom_fichier, contenu in documents_bruts.items():
        texte_nettoye = pretraitement(contenu)
        tokens = tokenisation(texte_nettoye)
        documents_tokenises[nom_fichier] = tokens
    return documents_tokenises


if __name__ == '__main__':
    # ğŸ”§ Mets ici le chemin de ton dossier contenant les fichiers texte :
    dossier_documents = 'D:\Bureau\TextIndexerPy\DATA'

    documents = charger_et_traiter(dossier_documents)

    for nom, tokens in documents.items():
        print(f"ğŸ“„ Fichier : {nom}")
        print(f"ğŸ”¢ Nombre de mots : {len(tokens)}")
        print(f"ğŸ“ Premiers tokens : {tokens[:10]}")
        print("-" * 40)
